---
title: 'Tutorial: Pay Equity Analysis [Part 1]'
author: "R package build"
date: '2022-01-17'
slug: tutorial-pay-equity-analysis-part-1
categories: People Analytics
tags:
- Python
- R
- Compensation
- DEI
- tutorial
subtitle: ''
summary: ''
authors: []
lastmod: '2022-01-18T22:31:54-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



```{r setup, include=FALSE, echo=FALSE}
library(reticulate)
use_python('/Users/elijaffe/opt/anaconda3/bin/python')

```


# Overview
In this tutorial, we will conduct a pay equity analysis on publicly available pay information for the City of Philadelphia. The goal is to present an example of how a pay equity analysis may be conducted that can be used in your organization. The tutorial consists of two main phases:  

1. Data Collection and Cleaning
 - This step uses Python for scraping the data from the web and collecting it into a workable file for analysis. We use a basic method to impute gender identification to give us a more real world example (note: this also limits the accuracy of our analysis. These results should not be interpreted to be an actual representation of pay equity among City of Philadelphia employees). Likely you will have direct access to the necessary data through your organization and this step will be unnecessary. Feel free to skip to the next section if so. However, if you are looking for a working data set to practice on your own, this section may be helpful.

2. Pay Equity Analysis
 - This is likely where you will start within your organization. I walk through two methods for analyzing pay data -- Regression as well as Classification and Regression Tree (CART) modeling. The analysis has been conducted using R.  


# Part 1 - Data Collecetion and Cleaning  


## Employee Salary  

The City of Philadelphia participates in an [Open Data program](opendataphilly.org) where they publish many different data sets related to the Philadelphia region, including information on pay for public employees. Using this resource, we can pull in the bulk of the data necessary to conduct a pay equity analysis. To begin with, let's import the necessary Python packages we will need. On top of the typical data science tools (pandas, numpy, seaborn, etc), we will be using `requests` for our API calls, `json` for parsing the data we get back, and `BeautifulSoup` for web scraping. There are a few auxiliary packages as well--go ahead and load them all so that everything works as intended.  


```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
sns.set()

import requests
import json
import ast
from bs4 import BeautifulSoup
from datetime import datetime
```


On the OpenDataPhilly.org website we can find Employee Earnings data by quarter, which has be provided with API access for us to use. We start by defining the URL to access the data. If you look at the URL, you will see the raw JSON. The requests package gets this JSON data and we next parse the JSON into the variable `data`.  


```{python}
URL = 'https://phl.carto.com/api/v2/sql?q=SELECT%20*%20FROM%20employee_earnings'
page = requests.get(URL)
j_data = page.content
data = json.loads(j_data)
```

Let's look at the variable `data`.

```{python}
# let's look at the variable data
type(data)

# we see that is a dict
# looking at the keys, we see there are 'rows', 'time', 'fields', and 'total_rows'
data.keys()
```

We see that is of type dict with the keys 'rows', 'time', 'fields', and 'total_rows'. Rows seems relevant to our needs, and we can confirm this by looking at a single observation.

```{python}
# rows seem like what we'll need
print(data['rows'][0])
```

```{python}
# pull this into a pandas dataframe
df = pd.DataFrame(data['rows'])
df.head()
```

Great, we now have the beginning of our data set. Let's take a look at what we have.

```{python}
# we have 23 columns and over 316,000 observations
df.info()
```
 


We see there are 23 columns and over 316,000 observations. Our variables are:

- **cartodb_id**: an index generated by cartodb (where we must have imported the data from)
- **the_geom** and **the_geom_webmercator**: geographic inforamtion. Not needed for our analysis
- **calendar_year**
- **quarter**: we must have pay info per quarter
- **last_name**
- **first_name**
- **title**
- **job_code**
- **department_name**
- **department_number**
- **base_salary**
- **salary_type** (Salaried or Non-Salaried. We will probably want to explore what the types are and how that affects `base_salary`)
- **overtime_gross_pay_qtd**: gross overtime pay that quarter
- **base_gross_pay_qtd**: base salary represented as a quarterly distribution
- **longevtiy_gross_pay_qtd** (Phila.gov must have some additional pay component reflecting tenure)
- **post_separation_gross_pay_qtd** (it appears Phila.gov will sometimes pay employees that have already left)
- **miscellaneous_gross_pay_qtd**
- **employee_category** (what are the categories here?)
- **compulsory_union_code**
- **termination_month**
- **termination_year**  

There's a lot here! The first few columns are identifiers from the database pull which we don't need to worry about for our analysis. Then we get into year/quarter, employee info including name, title, and department data, and then a bunch of different quarterly pay info. Finally, we have termination date if applicable. 

> At this point, I like to save the data to a file on my computer for easy access while I prepare it for analysis. This is not necessary, but ensures that I have the data in case anything happens to the source.  
>
>```{python}
df.to_csv(path_or_buf=f"{datetime.now().strftime('%Y-%m-%d')}_phila_ee_salary_data.csv", index=False)

# next time when we want to use this data, we can simply load the dataframe df from the csv file instead
# If you do so, un-comment the two lines below to continue following along
  
#df = pd.read_csv('[YOURDATE]_phila_ee_salary_data.csv')
#df.reset_index(drop = True, inplace=True)
```
>

It's a bit overwhelming to take this all in, so let's pare it down to just what we'll need. For our analysis, we will only look at salaried employees. The same employee appears multiple times if they've worked for more than one quarter, so lets select just the data from Q4 of 2020. We also want to look only at active employees so we will remove those with a termination date. Then, we'll drop the columns that aren't relevant for us.  


```{python}
# let's look at salaried employees only
df = df[df.salary_type == 'Salaried']

# we have data from more than one quarter. we want just the data from 2020Q4
df = df[df.calendar_year == 2020] 
df = df[df.quarter == 4]

# we also want only active employees (i.e. those without a termination month or year)
# check that all those with a termination month have a termination year (for data integrity purposes)

# drop any employee that has been terminated
df = df[df.termination_year.isna()]

# let us also drop some columns we know we won't be using
drop_cols = ['cartodb_id',
             'the_geom', 
             'the_geom_webmercator',
             'calendar_year',
             'quarter']

df.drop(drop_cols, axis=1, inplace=True)

df.head(20)
```


So far so good! We have a nice data set that we could use to answer a number of questions already. However, it will be more inciteful for pay equity if we can pull in a few additional variables that may impact pay.  


## Additional Job Information  


We now have a `df` with employee pay info but we do not know what the relevant job level and ranges are. We can find this on the phila.gov website. After a bit of digging, I found a page with [job spec info](https://www.phila.gov/personnel/Specs.html) online. With a tad of more advanced web scraping which is beyond the scope of this tutorial, I was able to pull the details into a json file which I load below. This file can be found in the github repository for the project as well. Follow along the steps below to clean the file you've just imported.

> Note: If you follow the link above, you'll notice there's a 404 error! When I initially pulled this data in April of 2021, the page was up. However, the Philly webpage must have changed in the meantime. Yet another reason to save a local copy of data acquired through webscraping so that when things change, you don't have to recreate the wheel.


```{python}
# I have pulled this into a json file which I open here
with open('2021_04_28_phila_master_data_pull.json') as fh:
    job_spec_data = json.load(fh)
    
# these are the rows we want for this analysis
data_subset = job_spec_data[12:986]

# we continue to pare down data_subset to just the details we need, in a nice dataframe-like list of dicts
# select just the relevant details (each data in subset is a key-value pair. We only want the value for each row)
for i, data in enumerate(data_subset):
    data_subset[i] = data_subset[i]['jobClassSpecs']

# this will be the DF we use! job_spec_df
job_spec_df = pd.DataFrame(data_subset)

# take a look at our work
job_spec_df.head()
```


In the dataframe `job_spec_df` we have the following variables: 

- **jobClassCode**
- **jobClassCode**
- **jobClassTitle**
- **payRange**
- **jobClassSalaryMin**
- **jobClassSalaryMax**
- **unionCode**
- **flsaCode**
- **effectiveDate**
- **family**  



The final step is to cast the data objects to the types we expect them to be.  

```{python}
# cast the dtypes to the correct dtype
job_spec_df['effectiveDate'] = pd.to_datetime(job_spec_df['effectiveDate'])
job_spec_df['family'] = job_spec_df['family'].astype(str)
job_spec_df['jobClassSalaryMin'] = job_spec_df['jobClassSalaryMin'].astype(float)
job_spec_df['jobClassSalaryMax'] = job_spec_df['jobClassSalaryMax'].astype(float)
```




## Imputing gender information

We have successfully gathered information on employee pay, job title, department, job type, union info and more. Working with a company, however, this is not necessarily riveting information. More often companies are interested in how demographics such as sex or race may affect these at an organizational level. For the purpose of this example, we take a guess an employee's biological sex to give us a mock data set.

<mark>Here it is worth reiterating that there are huge issues with doing this type of guesswork in a real world setting. Furthermore, we cannot use the analysis here to make any claims about the City of Philadelphia's actual pay practices. In this data set, employee gender is entirely a guess and we are not looking at gender non-conforming individuals.  *However* for the purposes of this exercise, I am happy with the number of observations and pretend gender. Now, we can attempt to determine pay inequity across gender based on this hypothetical data set. </mark> 

Another approach would have been to randomly assign gender to each observation in our data set, but I did not want to know ahead of time what the distributions would be. So, let's jump into it.  

We want a simple way of guessing at an employee's gender. To do this, we will parse a list of the 100 most common names by sex and cross reference it with our list of employees using the `BeautifulSoup` package. We use data on [Popular names for births in 1921-2020](https://www.ssa.gov/oact/babynames/decades/century.html) to help our guesses. Take a look at webpage--you can see a list with each of the most popular names by sex side by side. Looking at the html, we can see a clear pattern of where each name appears. This will make our web scraping easier.

```{python}
NAMES_URL = 'https://www.ssa.gov/oact/babynames/decades/century.html'
names_page = requests.get(NAMES_URL)

soup = BeautifulSoup(names_page.content, 'html.parser')
body = soup.tbody
```

`body` contains  parsed html. Using the `find.all` method we can grab each of the popular names and assign them to a corresponding list by sex.

```{python}
#initialize our lists
male_names = []
female_names = []

for line in body.find_all('tr'):
    for i, item in enumerate(line.find_all('td')):
        if i==1:
            male_names.append(item.text)
        elif i==3:    
            female_names.append(item.text)

# for male_names, the scaper captures an extra line with source info. We don't need this in our list
male_names = male_names[:100]
print(male_names)
```

In our employee data set, we will compare the first name of each employee to see if it appears in either our `female_names` list or our `male_names` list. If it does, we will assign them a binary value indicating which group they belong to.  

To help with our text comparisons, we want to make all letters lowercase so that capitalizations do not through off our script. Having done this, we iterate through our `df` and assign each employee to the variable `likely_female` or `likely_male`. Those employees who are not categorized by this method will be removed from our data.

```{python}
# first step will be to standardize lists to lowercase
for i, name in enumerate(male_names):
    male_names[i] = name.lower()

for i, name in enumerate(female_names):
    female_names[i] = name.lower()

df.first_name = df.first_name.str.lower()

# add likely_male and likely female columns to df, where a 1 represents yes and a 0 means no
df['likely_male'] = np.where(df.first_name.isin(male_names), 1, 0)
df['likely_female'] = np.where(df.first_name.isin(female_names), 1, 0)

df.head()
```

Let's see how well our matching process did.  

```{python}
# it appears we were able to guess about 50% of employees' gender
sum_males = sum(df['likely_male'])
sum_females = sum(df['likely_female'])

print('Count of likely males: ' + str(sum_males))
print('Count of likely females: ' + str(sum_females))
print('Total likely matches: ' + str(sum_males + sum_females))
print('Total observations in df: ' + str(len(df)))
print('Percentage counted: ' + str(np.round((sum_males + sum_females) / len(df) * 100,2)))
```

## Bringing it all together  


We have imputed sex information for about half of the employees in our data. Since we have a pretty high n, let's just toss out those who don't have sex info. Again, in a real organization, you would have this detail in your records so you would never face this issue.  


```{python}
# include just the observations of salaried employee that we have gender info for
df = df[(df['likely_female'] == 1) | (df['likely_male'] == 1)]

print(len(df))
```


We now just need to select the columns we will be using in our pay equity analysis, join the job level data we compiled in `job_spec_df`, and we'll be good to go!  


```{python}
df = df[['last_name',
          'first_name',
          'title',
          'job_code',
          'department_name',
          'department_number',
          'base_salary',
          'likely_male',
          'likely_female']]


# Now we have dataframe df which has employee data. we want to add payRange, jobClassSalaryMin, and jobClassSalaryMax to the data set
# mapped as job_code = jobClassCode

combined_df = df.merge(job_spec_df, how='left', left_on='job_code', right_on='jobClassCode')
```

Finally, let's take a look at the shape of our data and save to a file for our analysis.

```{python}
df_to_save = combined_df.dropna()

print(df_to_save.shape)
df_to_save.to_csv('MVR_preprocessed_data.csv', index=False)
```


# Conclusion
In part one of this Pay Equity Analysis tutorial, we pulled together the the necessary salary data using Philly's Open Data API. We then cleaned our job level data to help inform our analysis. Finally, we flexed our web scraping muscles to impute sex data and combined it with our employee pay data. We are now all set to do the actual pay equity analysis! You can find part two (coming soon) here.


<br>
<font color="grey"> Originally posted: January 18, 2022</font>